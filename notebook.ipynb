{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6504606,"datasetId":3758654,"databundleVersionId":6586949}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Vision Transformer (ViT) Model for RAF-DB Emotion Recognition\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.868919Z","iopub.execute_input":"2026-02-21T16:56:47.869222Z","iopub.status.idle":"2026-02-21T16:56:47.874158Z","shell.execute_reply.started":"2026-02-21T16:56:47.869193Z","shell.execute_reply":"2026-02-21T16:56:47.873495Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.893521Z","iopub.execute_input":"2026-02-21T16:56:47.893725Z","iopub.status.idle":"2026-02-21T16:56:47.897951Z","shell.execute_reply.started":"2026-02-21T16:56:47.893707Z","shell.execute_reply":"2026-02-21T16:56:47.897227Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Constants for RAF-DB (7 emotions)\nEMOTIONS = ['Surprise', 'Fear', 'Disgust', 'Happy', 'Sad', 'Anger', 'Neutral']\nNUM_CLASSES = len(EMOTIONS)\nBATCH_SIZE = 32\nEPOCHS = 20\nLEARNING_RATE = 1e-4\nIMG_SIZE = 224","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.901457Z","iopub.execute_input":"2026-02-21T16:56:47.901901Z","iopub.status.idle":"2026-02-21T16:56:47.912388Z","shell.execute_reply.started":"2026-02-21T16:56:47.901881Z","shell.execute_reply":"2026-02-21T16:56:47.911839Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Early stopping class\nclass EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.913634Z","iopub.execute_input":"2026-02-21T16:56:47.913948Z","iopub.status.idle":"2026-02-21T16:56:47.930002Z","shell.execute_reply.started":"2026-02-21T16:56:47.913924Z","shell.execute_reply":"2026-02-21T16:56:47.929069Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom pathlib import Path\nfrom PIL import Image\n\nclass RAFDBDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None):\n\n        self.root_dir = Path(root_dir) / split\n        self.transform = transform\n        self.images = []\n        self.labels = []\n\n        # RAF-DB has 7 emotion classes (1 ‚Üí 7)\n        for label in range(1, 8):\n\n            class_folder = self.root_dir / str(label)\n\n            if not class_folder.exists():\n                print(f\"Missing folder: {class_folder}\")\n                continue\n\n            # Load ALL aligned images\n            img_files = list(class_folder.glob('*_aligned.jpg'))\n\n            for img_path in img_files:\n                self.images.append(img_path)\n                self.labels.append(label - 1)  # make 0‚Äì6\n\n        print(f\"Loaded {len(self.images)} images for {split} split\")\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n\n        img_path = self.images[idx]\n        label = self.labels[idx]\n\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.931086Z","iopub.execute_input":"2026-02-21T16:56:47.931398Z","iopub.status.idle":"2026-02-21T16:56:47.945490Z","shell.execute_reply.started":"2026-02-21T16:56:47.931353Z","shell.execute_reply":"2026-02-21T16:56:47.944865Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Data transforms\ntrain_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.946886Z","iopub.execute_input":"2026-02-21T16:56:47.947087Z","iopub.status.idle":"2026-02-21T16:56:47.965732Z","shell.execute_reply.started":"2026-02-21T16:56:47.947070Z","shell.execute_reply":"2026-02-21T16:56:47.965146Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.966474Z","iopub.execute_input":"2026-02-21T16:56:47.966731Z","iopub.status.idle":"2026-02-21T16:56:47.980192Z","shell.execute_reply.started":"2026-02-21T16:56:47.966702Z","shell.execute_reply":"2026-02-21T16:56:47.979705Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Load datasets\ndata_dir = \"/kaggle/input/datasets/shuvoalok/raf-db-dataset/DATASET\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.980994Z","iopub.execute_input":"2026-02-21T16:56:47.981255Z","iopub.status.idle":"2026-02-21T16:56:47.992259Z","shell.execute_reply.started":"2026-02-21T16:56:47.981230Z","shell.execute_reply":"2026-02-21T16:56:47.991768Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(\"Loading RAF-DB datasets...\")\ntrain_dataset = RAFDBDataset(data_dir, split='train', transform=train_transform)\ntest_dataset = RAFDBDataset(data_dir, split='test', transform=test_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:47.993500Z","iopub.execute_input":"2026-02-21T16:56:47.993695Z","iopub.status.idle":"2026-02-21T16:56:48.100750Z","shell.execute_reply.started":"2026-02-21T16:56:47.993678Z","shell.execute_reply":"2026-02-21T16:56:48.100192Z"}},"outputs":[{"name":"stdout","text":"Loading RAF-DB datasets...\nLoaded 12271 images for train split\nLoaded 3068 images for test split\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:48.101447Z","iopub.execute_input":"2026-02-21T16:56:48.101667Z","iopub.status.idle":"2026-02-21T16:56:48.106618Z","shell.execute_reply.started":"2026-02-21T16:56:48.101639Z","shell.execute_reply":"2026-02-21T16:56:48.106161Z"}},"outputs":[{"name":"stdout","text":"Training samples: 12271\nTest samples: 3068\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Initialize Vision Transformer model\nprint(\"\\nInitializing Vision Transformer model...\")\nvit = models.vit_b_16(pretrained=True)\nnum_features = vit.heads.head.in_features\nvit.heads.head = nn.Linear(num_features, NUM_CLASSES)\nvit = vit.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T16:56:48.107778Z","iopub.execute_input":"2026-02-21T16:56:48.107976Z","iopub.status.idle":"2026-02-21T16:56:49.141946Z","shell.execute_reply.started":"2026-02-21T16:56:48.107958Z","shell.execute_reply":"2026-02-21T16:56:49.141147Z"}},"outputs":[{"name":"stdout","text":"\nInitializing Vision Transformer model...\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Training function with early stopping\ndef train_vit(model, model_name, train_loader, test_loader, epochs=EPOCHS, patience=5):\n    print(f\"\\n{'='*50}\")\n    print(f\"Training {model_name} with Early Stopping (patience={patience})\")\n    print('='*50)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n    \n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n    \n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    best_val_acc = 0.0\n    stopped_epoch = epochs\n    \n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n        for inputs, labels in train_pbar:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            train_pbar.set_postfix({'loss': running_loss/(len(train_pbar)), \n                                   'acc': 100.*correct/total})\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            val_pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n            for inputs, labels in val_pbar:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                \n                val_pbar.set_postfix({'loss': val_loss/(len(val_pbar)), \n                                     'acc': 100.*correct/total})\n        \n        val_loss = val_loss / len(test_loader)\n        val_acc = 100. * correct / total\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        scheduler.step(val_loss)\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), f'best_{model_name}_rafdb.pth')\n            print(f\"‚úì New best model saved with accuracy: {val_acc:.2f}%\")\n        \n        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n        \n        # Check early stopping\n        early_stopping(val_loss)\n        if early_stopping.early_stop:\n            print(f\"\\n‚ö†Ô∏è Early stopping triggered at epoch {epoch+1}\")\n            stopped_epoch = epoch + 1\n            break\n    \n    # Final evaluation with best model\n    print(f\"\\nüìä Loading best model for final evaluation...\")\n    model.load_state_dict(torch.load(f'best_{model_name}_rafdb.pth'))\n    model.eval()\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    return {\n        'model_name': model_name,\n        'train_losses': train_losses,\n        'train_accs': train_accs,\n        'val_losses': val_losses,\n        'val_accs': val_accs,\n        'best_val_acc': best_val_acc,\n        'predictions': all_preds,\n        'true_labels': all_labels,\n        'stopped_epoch': stopped_epoch\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train Vision Transformer\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING VISION TRANSFORMER TRAINING ON RAF-DB\")\nprint(\"=\"*60)\n\nvit_results = train_vit(vit, 'ViT_RAFDB', train_loader, test_loader, patience=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting results for Vision Transformer\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nepochs_range = range(1, len(vit_results['train_losses']) + 1)\n\n# Plot losses\naxes[0].plot(epochs_range, vit_results['train_losses'], label='Train Loss', marker='o', color='purple')\naxes[0].plot(epochs_range, vit_results['val_losses'], label='Val Loss', marker='s', color='orange')\naxes[0].axvline(x=vit_results['stopped_epoch'], color='red', linestyle='--', alpha=0.7, label='Early Stop')\naxes[0].set_title(f'Vision Transformer on RAF-DB - Loss (Stopped at epoch {vit_results[\"stopped_epoch\"]})')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Plot accuracies\naxes[1].plot(epochs_range, vit_results['train_accs'], label='Train Acc', marker='o', color='purple')\naxes[1].plot(epochs_range, vit_results['val_accs'], label='Val Acc', marker='s', color='orange')\naxes[1].axvline(x=vit_results['stopped_epoch'], color='red', linestyle='--', alpha=0.7, label='Early Stop')\naxes[1].set_title('Vision Transformer on RAF-DB - Accuracy')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy (%)')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('vit_rafdb_training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion matrix for Vision Transformer\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(vit_results['true_labels'], vit_results['predictions'])\n\n# Normalize confusion matrix\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Plot\nsns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Purples', \n            xticklabels=EMOTIONS, yticklabels=EMOTIONS)\nplt.title('Vision Transformer on RAF-DB - Normalized Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.savefig('vit_rafdb_confusion_matrix.png', dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification report for Vision Transformer\nprint(\"\\n\" + \"=\"*60)\nprint(\"VISION TRANSFORMER ON RAF-DB - CLASSIFICATION REPORT\")\nprint(\"=\"*60)\nprint(classification_report(vit_results['true_labels'], vit_results['predictions'], \n                           target_names=EMOTIONS, zero_division=0, digits=4))\n\n# Per-class performance analysis\nprint(f\"\\nüìà Detailed Analysis for Vision Transformer on RAF-DB:\")\ncm = confusion_matrix(vit_results['true_labels'], vit_results['predictions'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate per-class accuracy\nclass_acc = cm.diagonal() / cm.sum(axis=1)\nprint(f\"\\n{'Emotion':<15} {'Accuracy':<15}\")\nprint(\"-\"*30)\nfor emotion, acc in zip(EMOTIONS, class_acc):\n    print(f\"{emotion:<15} {acc*100:.2f}%\")\n\n# Find easiest and hardest emotions\nbest_class_idx = np.argmax(class_acc)\nworst_class_idx = np.argmin(class_acc)\nprint(f\"\\n‚úÖ Best performing emotion: {EMOTIONS[best_class_idx]} ({class_acc[best_class_idx]*100:.2f}%)\")\nprint(f\"‚ùå Worst performing emotion: {EMOTIONS[worst_class_idx]} ({class_acc[worst_class_idx]*100:.2f}%)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}